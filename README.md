# pittgoogle-user

This repo contains examples and tutorials for accessing astronomical data that is served by the Pitt-Google Alert Broker.
This data includes alert streams produced by the Zwicky Transient Facility (ZTF) and the LIGO-Virgo-KAGRA (LVK) Collaboration.
The data is available in Google Cloud in several formats:

- Live streams of alert data (Google Pub/Sub). Current streams: ZTF, LVK. Planned: Rubin LSST.
- Alert archive as tables that support SQL queries and are updated in real time (Google BigQuery).
- Alert archive as files in object storage (Google Cloud Storage).

Data can be further processed by you using Google Cloud resources or downloaded/exported to a location of your choosing.

The mission of the Pitt-Google Alert Broker is to provide the scientific community with broad access to the real-time astronomical alert streams produced by current and planned missions like ZTF, LVK, and Rubin Observatory's upcoming Legacy Survey of Space and Time (LSST).
in ways that support the scientific community to access, analyze, and make discoveries in the growing wealth of data being generated by these surveys.
This is due to our use of Google Pub/Sub to serve the live alert streams (in place of the more commonly used Apache Kafka) which makes the data available to the community in new ways.
Live alert streams and archival data served by Pitt-Google can be accessed and processed both from within the Google Cloud and outside of it.

We anticipate that science use cases which are especially time-sensitive, such as gravitational wave follow up in search of kilonovae and other exotic objects, can benefit greatly from Pub/Sub access to the alert streams, particularly by coupling it with Google's container services like Cloud Run.

## Examples

The following examples are included in this repo.
Before you begin, complete the [one-time setup](one-time-setup.md).

### Cloud Run

Use case : Process alert streams in real time

- Write code that runs your analysis (e.g., classifier) on a single alert from ZTF, LVK, etc.
- Deploy a container to Google Cloud that executes your code in real time, as the alerts are published by the survey.

### BigQuery

Use case : Query tables of alert data that are maintained by Pitt-Google Broker and updated in real time.

Use case : Store your own tabular data

## Overview:
The Pitt-Google Broker is a cloud-based alert distribution service designed to provide near real-time processing of data from large-scale astronomical surveys like the [Legacy Survey of Space and Time](https://www.lsst.org). For its participation in the [DESC ELAsTiCC Challenge](https://portal.nersc.gov/cfs/lsst/DESC_TD_PUBLIC/ELASTICC/), the Pitt-Google Broker team developed this repo to process and classify alerts streamed by ELAsTiCC. Alerts are ingested into the data pipeline and classified using [SuperNNova](https://supernnova.readthedocs.io/en/latest/index.html). Supernovae classifications are published to a Pub/Sub topic and subsequently stored in BigQuery.

The deployment script in this repo can be used to create and delete some of the aforementioned Google Cloud Platform resources:

* A Pub/Sub subscription:
    * Used to trigger a Cloud Run instance
* A container image of the classifier module, which is then deployed using Cloud Run
* A Pub/Sub topic:
    * Used to write SuperNNova classifications to a BigQuery table
* A BigQuery dataset & table:
    * Used to store SuperNNova classifications

## How to use the deployment script
* Begin a new terminal and confirm that your current directory is the same as the classifier's root directory (e.g., pittgoogle-user/SuperNNova).
* Initialize the following variables:

    ```
    testid="<enter testid name>" (default: test)

    survey="enter survey name"

    teardown="<enter False>" (default: False)

    trigger_topic="<enter trigger topic name>" (default: elasticc-alerts)

    trigger_topic_project="<enter trigger topic project name>" (default: avid-heading-329016)
    ```


* NOTE: If a Cloud Run instance already exists, selecting `teardown="True"` will delete all GCP resources associated with the information provided above.


* To call the deployment script, enter the following command:
    ```
    ./setup.sh "$testid" "$survey" "$teardown" "$trigger_topic" "$trigger_topic_project"
    ```
    * As the deployment script executes, a series of messages will appear. These messages will describe the status of deployment.


* Once the deployment script has successfully created your Cloud Run instance, you can review its status, and the status of other instances on the [Google Cloud Console](https://console.cloud.google.com/run?). Verify that the number of requests per second (Req/sec) is non-zero.

## How to stop a Cloud Run module
* Cloud Run instances are triggered by a Pub/Sub subcription. To stop a Cloud Run instance without deleting it, delete the trigger subscription. This can be done on the [Google Cloud Console](https://console.cloud.google.com/cloudpubsub/subscription/list?) or on the command line using: `gcloud pubsub subscriptions delete <enter subscription name>`.

* To view which subscription is triggering your Cloud Run instance, select your instance from the [Google Cloud Console](https://console.cloud.google.com/run?) and locate the _Triggers_ tab (it is located under the _URL_ section).

## How to delete a Cloud Run instance & all associated GCP resources
* The procedure follows the first few steps of this tutorial. Begin a new terminal and initialize the same variables. This time, select `teardown="True"`

* Confirm that your current directory is the same as the repo's root directory, and use the command `./setup.sh "$testid" "$survey" "$teardown" "$trigger_topic" "$trigger_topic_project"` to delete all the GCP resources associated with the information you provided (e.g., `testid`, `survey`, `trigger_topic`, etc.).
